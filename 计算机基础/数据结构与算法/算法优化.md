## 数据结构为何重要
哪怕只写过几行代码的人都会发现，编程基本上就是在跟数据打交道。计算机程序总是在接收数据、操作数据或返回数据

**数据结构**则是指数据的组织形式

**数据结构**不只是用于组织数据，它还极大地影响着代码的运行速度。因为数据结构不同，程序的运行速度可能相差多个数量级。如果你写的程序要处理大量的数据，或者要让数千人同时使用，那么你采用何种数据结构，将决定它是能够运行，还是会因为不堪重负而崩溃

一旦对各种数据结构有了深刻的理解，并明白它们对程序性能方面的影响，你就能写出快速而优雅的代码，从而使软件运行得快速且流畅。当然，你的编程技能也会更上一层楼

### 基础数据结构: 数组
**数组**是计算机科学中最基本的数据结构之一

若想了解某个数据结构的性能，得分析程序怎样操作这一数据结构

一般数据结构都有以下4种操作:
- 读取
- 查找
- 插入
- 删除

第一个重要理论: ***操作的速度，并不按时间计算，而是按步数计算**

因为，你不可能很绝对地说，某项操作要花5秒。它在某台机器上要跑5秒，但换到一台旧一点的机器，可能就要多于5秒，而换到一台未来的超级计算机，运行时间又将显著缩短。所以，受硬件影响的计时方法，非常不可靠

然而，若按步数来算，则确切得多。如果A操作要5步，B操作要500步，那么我们可以很肯定地说，无论是在什么样的硬件上对比，A都快过B。因此，衡量步数是分析速度的关键

此外，操作的速度，也常被称为**时间复杂度**。在本书中，我们会提到速度、时间复杂度、效率、性能，但它们其实指的都是*步数**

在数组中，这种逐个去检查的做法，就是最基本的查找方法——**线性查找**


### 集合: 一条规则决定性能
集合：**它是一种不允许元素重复的数据结构**

其实集合是有不同形式的，但现在我们只讨论基于数组的那种。这种集合跟数组差不多，都是一个普通的元素列表，唯一的区别在于，**集合不允许插入重复的值**

集合就是用于确保数据不重复

**理解数据结构的性能，关键在于分析操作所需的步数**

## 算法为何重要
数据结构确定了，代码的速度也还会受另一重要因素影响，那就是**算法**

**算法只是解决某个问题的一套流程**

不同的数据结构有不同的时间复杂度，类似地，不同的算法(即使是用在同一种数据结构上)也有不同的时间复杂度。既然我们已经学会了**时间复杂度**的分析方法，那么现在就可以用它来对比各种算法，找出能够发挥代码极限性能的那个

### 有序数组
有序数组跟上面讨论的数组几乎一样，唯一区别就是有序数组要求其值总是保持有序。即每次插入新值时，它会被插入到适当的位置，使整个数组的值仍然按顺序排列。常规的数组则并不考虑是否有序，直接把值加到末尾也没问题

往有序数组中插入新值，需要先做一次查找以确定插入的位置。这是它跟常规数组的关键区别(在性能方面)之一

虽然插入的性能比不上常规数组，但在查找方面，有序数组却有着特殊优势

### 查找有序数组
上面介绍了常规数组的查找方式：从左至右，逐个格子检查，直至找到。这种方式称为**线性查找**

接下来看看有序数组的线性查找跟常规数组有何不同:
- 设一个常规数组`[17,3,75,202,80]`，如果想在里面查找`22`（其实并不存在），那你就得逐个元素去检查，因为`22`可能在任何一个位置上。要想在到达末尾之前结束检查，那么所找的值必须在末尾之前出现
- 然而对于有序数组来说，即便它不包含要找的值，我们也可以提早停止查找。假设要在有序数组`[3,17,75,80,202]`里查找`22`，我们可以在查到`75`的时候就结束，因为`22`不可能出现在`75`的右边

Go语言实现有序数组线性查找:
```
package main

import "fmt"

func linear_search(arr []int, val int) int {
	if len(arr) == 0 {
		return -1
	}
	for _, v := range arr {
		if v == val {
			return val
		} else if v > val {
			break
		}
	}
	return -1
}

func main() {
	arr := []int{3, 17, 75, 80, 202}
	value := linear_search(arr, 80)
	fmt.Println(value)
}
```

因此，有序数组的线性查找大多数情况下都会快于常规数组。除非要找的值是最后那个，或者比最后的值还大，那就只能一直查到最后了

只看到这里的话，可能你还是不会觉得两种数组在性能上有什么巨大区别

至今我们提到的查找有序数组的方法就只有**线性查找**。但其实，**线性查找**只不过是查找算法的其中一种而已

有序数组相比常规数组的一大优势就是它可以使用另一种查找算法。此种算法名为**二分查找**，它比线性查找要快得多

### 二分查找
有序数组相比常规数组的一大优势就是它除了可以用线性查找，还可以用**二分查找**。常规数组因为无序，所以不可能运用二分查找

```
package main

import "fmt"

func binarySearch(arr []int, k int) int {
	l := 0
	r := len(arr) - 1

	for l <= r {
		mid := (l + r) / 2
		if k == arr[mid] {
			return mid
		}
		if k < arr[mid] {
			r = mid - 1
		} else {
			l = mid + 1
		}
	}
	return -1
}

func main() {
	arr := []int{3, 17, 75, 80, 202}
	value := binarySearch(arr, 80)
	fmt.Println(value)
}
```

### 二分查找与线性查找
对于长度太小的有序数组，二分查找并不比线性查找好多少。但我们来看看更大的数组:

对于拥有100个值的数组来说，两种查找需要的最多步数如下所示:
- 线性查找,100步
- 二分查找,7步

**二分查找会在每次猜测后排除掉一半的元素**

不过还要记住，有序数组并不是所有操作都比常规数组要快。如你所见，它的插入就相对要慢。衡量起来，虽然插入是慢了一些，但查找却快了许多。还是那句话，**你得根据应用场景来判断哪种更合适**


----------------

很多时候，计算一样东西并不只有一种方法，换种算法可能会极大地影响程序的性能

同时你还应意识到，世界上并没有哪种适用于所有场景的数据结构或者算法。你不能因为有序数组能使用二分查找就永远只用有序数组。在经常插入而很少查找的情况下，显然插入迅速的常规数组会是更好的选择

如之前所述，比较算法的方式就是**比较各自的步数**

下面，我们将会学习如何规范地描述数据结构和算法的时间复杂度。有了这种通用的表达方式，就能更容易地观察出哪种算法符合我们的实际需求

## 大 O 记法
从之前的内容中得知，影响算法性能的主要因素是其所需的**步数**

量化线性查找效率的更准确的方式应该是:**对于具有n个元素的数组，线性查找最多需要n步**

为了方便表达数据结构和算法的**时间复杂度**，计算机科学家从数学界借鉴了一种简洁又通用的方式，那就是`大O记法`。这种规范化语言使得我们可以轻松地指出一个算法的性能级别，也令学术交流变得简单

掌握了`大O记法`，就掌握了算法分析的专业工具

### 大O:数步数
为了统一描述，`大O`不关注算法所用的时间，只关注其所用的**步数**

数组不论多大，读取都只需1步。用`大O记法`来表示，就是: `O(1)`

`O(1)`意味着一种算法无论面对多大的数据量，其步数总是相同的。就像无论数组有多大，读取元素都只要1步。这1步在旧机器上也许要花20分钟，而用现代的硬件却只要1纳秒。但这两种情况下，读取数组都是1步

下面研究一下`大O记法`如何描述线性查找的效率。回想一下，线性查找在数组上要逐个检查每个格子。在最坏情况下，线性查找所需的步数等于格子数。即如前所述：对于n个元素的数组，线性查找需要花n步,用`大O记法`来表示，即为: `O(n)`

### 常数时间与线性时间
从 `O(n)`可以看出，`大O记法`不只是用固定的数字（如22、440）来表示算法的步数，而是基于要处理的数据量来描述算法所需的步数。或者说，`大O`解答的是这样的问题：当数据增长时，步数如何变化?

`O(n)`算法所需的步数等于数据量，意思是当数组增加一个元素时，`O(n)`算法就要增加1步。而`O(1)`算法无论面对多大的数组，其步数都不变

`O(n)`呈现为一条对角线。当数据增加一个单位时，算法也随之增加一步。也就是说，数据越多，算法所需的步数就越多。`O(n)`也被称为**线性时间**

相比之下，`O(1)`则为一条水平线，因为不管数据量是多少，算法的步数都恒定。所以，`O(1)`也被称为**常数时间**

因为不管数据量怎样变化，算法的步数都恒定，所以这也是常数时间，也可以表示为`O(1)`。虽然从技术上来说它需要3步而不是1步，但`大O记法`并不纠结于此。简单来说，**O(1)就是用来表示所有数据增长但步数不变的算法**

### 同一算法，不同场景
线性查找并不总是`O(n)`的。当要找的元素在数组末尾，那确实是`O(n)`。但如果它在数组开头，1步就能找到的话，那么技术上来说应该是`O(1)`。所以概括来说，线性查找的最好情况是`O(1)`，最坏情况是`O(n)`

虽然`大O`可以用来表示给定算法的最好和最坏的情景，但若无特别说明，`大O记法`一般都是指最坏情况。因此尽管线性查找有`O(1)`的最好情况，但大多数资料还是把它归类为`O(n)`

这种悲观主义其实是很有用的:知道各种算法会差到什么程度，能使我们做好最坏打算，以选出最适合的算法

### 第三种算法
在同一个有序数组里，二分查找比线性查找要快。下面就来看看如何用`大O记法`描述二分查找

它不能写成 `O(1)`，因为二分查找的步数会随着数据量的增长而增长。它也不能写成 `O(n)`，因为步数比元素数量要少得多，包含100个元素的数组只要7步就能找完

看来，二分查找的时间复杂度介于`O(1)`和`O(n)`之间

二分查找的大O记法是: `O(logn)`, 归于此类的算法，它们的时间复杂度都叫作**对数时间**

简单来说，`O(logn)`意味着该算法当数据量翻倍时，步数加1。这确实符合之前我们所介绍的**二分查找**

到这里我们所提过的3种时间复杂度，按照效率由高到低来排序的话，会是这样:
1. O(1)
2. O(logn)
3. O(n)

![时间复杂度对比图](./img/时间复杂度对比图.PNG "时间复杂度对比图")

### 对数
**对数是指数的反函数**，所以我们先回顾一下指数

`2^3` 等于: `2*2*2`, 结果为 `8`

`log2^8`则将上述计算反过来，它意思是：**要把2乘以自身多少次，才能得到8**。因为需要3次，所以，`log2^8=3`


### 解释O(logn)
现在回到`大O记法`。当我们说`O(logn)`时，其实指的是`O(log2^n)`，不过为了方便就省略了2而已

你应该还记得O(n)代表算法处理n个元素需要n步。如果元素有8个，那么这种算法就需要8步

`O(logn)`则代表算法处理n个元素需要`log2^n`步。如果有8个元素，那么这种算法需要3步，因为`log2^8=3`


从另一个角度来看，如果要把8个元素不断地分成两半，那么得拆分3次才能拆到只剩1个元素

这正是二分查找所干的事情。它就是不断地将数组拆成两半，直至范围缩小到只剩你要找的那个元素

简单来说，O(log2^N)算法的步数等于二分数据直至元素剩余1个的次数

下表是`O(N)`和`O(log2^N)`的效率对比:

![n与logn对比图](./img/n与logn对比图.PNG "n与logn对比图")

每次数据量翻倍时，`O(N)`算法的步数也跟着翻倍，`O(log2^N)`算法却只需加1

---------

学会`大O记法`，我们在比较算法时就有了一致的参考系。有了它，我们就可以在现实场景中测量各种数据结构和算法，写出更快的代码，更轻松地应对高负荷的环境

## 运用大O来给代码提速
